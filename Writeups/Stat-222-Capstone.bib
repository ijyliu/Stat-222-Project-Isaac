
@article{das_credit_2023,
	title = {Credit {Risk} {Modeling} with {Graph} {Machine} {Learning}},
	volume = {2},
	issn = {2694-4022},
	url = {https://pubsonline.informs.org/doi/full/10.1287/ijds.2022.00018},
	doi = {10.1287/ijds.2022.00018},
	abstract = {Accurate credit ratings are an essential ingredient in the decision-making process for investors, rating agencies, bond portfolio managers, bankers, and policy makers, as well as an important input for risk management and regulation. Credit ratings are traditionally generated from models that use financial statement data and market data, which are tabular (numeric and categorical). Using machine learning methods, we construct a network of firms using U.S. Securities and Exchange Commission (SEC) filings (denoted CorpNet) to enhance the traditional tabular data set with a corporate graph. We show that this generates accurate rating predictions with comparable and better performance to tabular models. We ensemble graph convolutional networks with highly-performant ensembled machine learning models using AutoGluon. This paper demonstrates both transductive and inductive methodologies to extend credit scoring models based on tabular data, which have been used by the ratings industry for decades, to the class of machine learning models on networks. The methodology is extensible to other financial machine learning models that may be enhanced using a corporate graph.},
	number = {2},
	urldate = {2024-03-25},
	journal = {INFORMS Journal on Data Science},
	author = {Das, Sanjiv and Huang, Xin and Adeshina, Soji and Yang, Patrick and Bachega, Leonardo},
	month = oct,
	year = {2023},
	note = {Publisher: INFORMS},
	keywords = {corporate graph, credit ratings, graph neural networks, machine learning},
	pages = {197--217},
	file = {Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\UDSYMFB9\\Das et al. - 2023 - Credit Risk Modeling with Graph Machine Learning.pdf:application/pdf},
}

@misc{s_and_p_global_ratings_s_2024,
	title = {S and {P} {Global} {Ratings}},
	url = {https://www.spglobal.com/ratings/en/},
	abstract = {We provide intelligence that is embedded into the workflow and decision-making of customers around the globe.},
	language = {en-us},
	urldate = {2024-03-25},
	author = {{S and P Global Ratings}},
	year = {2024},
}

@misc{makwana_corporate_2022,
	title = {Corporate {Credit} {Rating}},
	url = {https://www.kaggle.com/datasets/agewerc/corporate-credit-rating},
	abstract = {Credit Ratings of Big US Firms and their Financials},
	language = {en},
	urldate = {2024-03-25},
	author = {Makwana, Ravi and Bhatt, Dhruvil and Delwadia, Kirtan},
	month = jun,
	year = {2022},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\Q2KQCMHW\\corporate-credit-rating.html:text/html},
}

@misc{gewerc_corporate_2020,
	title = {Corporate {Credit} {Rating} with {Financial} {Ratios}},
	url = {https://www.kaggle.com/datasets/kirtandelwadia/corporate-credit-rating-with-financial-ratios},
	abstract = {Credit ratings of all publicly traded US companies with financial ratios},
	language = {en},
	urldate = {2024-03-25},
	author = {Gewerc, Alan},
	month = jul,
	year = {2020},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\V3MJX6YE\\corporate-credit-rating-with-financial-ratios.html:text/html},
}

@misc{financial_modeling_prep_financial_2024,
	title = {Financial {Modeling} {Prep} - {FinancialModelingPrep}},
	url = {https://site.financialmodelingprep.com},
	abstract = {FMP offers a stock market data API covers real-time stock prices, historical prices and market news to stock fundamentals and company information.},
	language = {en},
	urldate = {2024-03-25},
	journal = {Financial Modeling Prep - FinancialModelingPrep},
	author = {{Financial Modeling Prep}},
	year = {2024},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\XXFSTC5G\\site.financialmodelingprep.com.html:text/html},
}

@misc{makwana_understanding_2022,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Understanding and {Attaining} an {Investment} {Grade} {Rating} in the {Age} of {Explainable} {AI}},
	url = {https://papers.ssrn.com/abstract=4163283},
	doi = {10.2139/ssrn.4163283},
	abstract = {Specialized agencies issue corporate credit ratings to evaluate the creditworthiness of a company, serving as a crucial financial indicator for potential investors. These ratings offer a tangible understanding of the risks associated with the credit investment returns of a company. Every company aims to achieve a favorable credit rating, as it enables them to attract more investments and reduce their cost of capital. Credit rating agencies typically employ unique rating scales that are broadly categorized into investment-grade or non-investment-grade (junk) classes. Given the extensive assessment conducted by credit rating agencies, it becomes a challenge for companies to formulate a straightforward and all-encompassing set of rules which may help to understand and improve their credit rating. This paper employs explainable AI, specifically decision trees, using historical data to establish an empirical rule on financial ratios. The rule obtained using the proposed approach can be effectively utilized to understand as well as plan and attain an investment-grade rating. Additionally, the study investigates the temporal aspect by identifying the optimal time window for training data. As the availability of structured data for temporal analysis is currently limited, this study addresses this challenge by creating a large and high-quality curated dataset. This dataset serves as a valuable resource for conducting comprehensive temporal analysis. Our analysis demonstrates that the empirical rule derived from historical data, yields a high precision value, and therefore highlights the effectiveness of our proposed approach as a valuable guideline and a feasible decision support system.},
	language = {en},
	urldate = {2024-03-26},
	author = {Makwana, Ravi and Bhatt, Dhruvil and Delwadia, Kirtan and Shah, Agam and Chaudhury, Bhaskar},
	month = jul,
	year = {2022},
	keywords = {Credit Rating, Data Visualization, Decision Tree, ETL, Explainable AI, Exploratory Data Analysis (EDA)},
}

@misc{mamaysky_credit_2023,
	title = {Credit {Information} in {Earnings} {Calls}},
	url = {http://arxiv.org/abs/2209.11914},
	abstract = {We develop a novel technique to extract credit-relevant information from the text of quarterly earnings calls. This information is not spanned by fundamental or market variables and forecasts future credit spread changes. One reason for such forecastability is that our text-based measure predicts future credit spread risk and firm profitability. More firm- and call-level complexity increase the forecasting power of our measure for spread changes. Out-of-sample portfolio tests show the information in our measure is valuable for investors. Both results suggest that investors do not fully internalize the credit-relevant information contained in earnings calls.},
	urldate = {2024-03-26},
	publisher = {arXiv},
	author = {Mamaysky, Harry and Shen, Yiwen and Wu, Hongyu},
	month = sep,
	year = {2023},
	note = {arXiv:2209.11914 [q-fin]
version: 2},
	keywords = {Quantitative Finance - General Finance},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\MG4SPCIP\\2209.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\PZDV7VBF\\Mamaysky et al. - 2023 - Credit Information in Earnings Calls.pdf:application/pdf},
}

@article{donovan_measuring_2021,
	title = {Measuring credit risk using qualitative disclosure},
	volume = {26},
	issn = {1573-7136},
	url = {https://doi.org/10.1007/s11142-020-09575-4},
	doi = {10.1007/s11142-020-09575-4},
	abstract = {We use machine learning methods to create a comprehensive measure of credit risk based on qualitative information disclosed in conference calls and in management’s discussion and analysis section of the 10-K. In out-of-sample tests, we find that our measure improves the ability to predict credit events (bankruptcies, interest spreads, and credit rating downgrades), relative to credit risk measures developed by prior research (e.g., z-score). We also find our measure based on conference calls explains within-firm variation in future credit events; however, we find little evidence that the measures of credit risk developed by prior research explain within-firm variation in credit risk. Our measure has utility for both academics and practitioners, as the majority of firms do not have readily available measures of credit risk, such as actively-traded CDS or credit ratings. Our study also adds to the growing body of research using machine-learning methods to gather information from conference calls and MD\&A to explain key outcomes.},
	language = {en},
	number = {2},
	urldate = {2024-03-26},
	journal = {Review of Accounting Studies},
	author = {Donovan, John and Jennings, Jared and Koharki, Kevin and Lee, Joshua},
	month = jun,
	year = {2021},
	keywords = {Credit risk, Disclosure, G20, G23, G30, G32, G33, M40, M41, Machine-learning, Textual analysis},
	pages = {815--863},
}

@article{he_impact_2018,
	title = {The {Impact} of {Impending} {Credit} {Rating} {Changes} on {Management} {Earnings} {Forecasts}},
	volume = {18},
	doi = {10.17406/GJMBRCVOL18IS4PG1},
	abstract = {This study investigates whether impending credit rating changes affect managers’ voluntary financial disclosure behaviors. I find that firms near a credit rating change do not opportunistically alter their financial disclosure practices to manipulate rating agencies’ perceptions of corporate credit risk. In particular, firms close to a credit rating change do not selectively release good news or withhold bad news on their earnings information. Nor do the firms likely issue an optimistically biased forecast or a more precise forecast for good news than for bad news. Overall, there is no evidence suggesting that credit ratings are manipulated via management earnings forecasts.},
	journal = {Global Journal of Management and Business Research},
	author = {He, Guanming},
	month = jun,
	year = {2018},
	pages = {1--18},
	file = {Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\X5PURZWK\\He - 2018 - The Impact of Impending Credit Rating Changes on M.pdf:application/pdf},
}

@article{khorram_information_2023,
	title = {Information flow and credit rating announcements},
	volume = {65},
	issn = {1386-4181},
	url = {https://www.sciencedirect.com/science/article/pii/S1386418123000356},
	doi = {10.1016/j.finmar.2023.100837},
	abstract = {We employ the implied volatility spread (IVS) and the short lending fee as measures of private information conveyed by their respective markets. Using issuer credit rating announcements as an informational event, we find that both IVS and the short fee have significantly higher predictive power for returns on event days versus non-event days. Both also predict the direction and magnitude of credit rating changes. Consistent with the linkage between the short sale and options markets, in models with both explanatory variables, the short fee remains significant in all specifications, while IVS loses explanatory power.},
	urldate = {2024-03-26},
	journal = {Journal of Financial Markets},
	author = {Khorram, Mehdi and Mo, Haitao and Sanger, Gary C.},
	month = sep,
	year = {2023},
	keywords = {Credit rating announcements, Implied volatility spread, Options market, Return predictability, Stock lending market},
	pages = {100837},
	file = {ScienceDirect Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\RXX9HUND\\S1386418123000356.html:text/html},
}

@article{shivakumar_debt_2011,
	title = {The debt market relevance of management earnings forecasts: evidence from before and during the credit crisis},
	volume = {16},
	issn = {1573-7136},
	shorttitle = {The debt market relevance of management earnings forecasts},
	url = {https://doi.org/10.1007/s11142-011-9155-6},
	doi = {10.1007/s11142-011-9155-6},
	abstract = {We investigate the credit market’s response via changes in credit default swap (CDS) spreads to management earnings forecasts and evaluate the importance of these forecasts relative to earnings news during the periods before and during the recent credit crisis. We document that credit markets react significantly to management forecast news and that the reactions to forecast news are stronger than to actual earnings news. Consistent with the asymmetric payoffs to debt holders, the forecast news is mainly relevant for firms with poor credit rating or announcing bad news. We also show that the relevance of management forecasts to credit markets is particularly strong during periods of high uncertainty, as experienced during the recent credit crisis.},
	language = {en},
	number = {3},
	urldate = {2024-03-26},
	journal = {Review of Accounting Studies},
	author = {Shivakumar, Lakshmanan and Urcan, Oktay and Vasvari, Florin P. and Zhang, Li},
	month = sep,
	year = {2011},
	keywords = {M41, Credit crisis, Credit default swaps, Earnings announcements, G01, G14, Management earnings forecasts},
	pages = {464--486},
	file = {Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\TVFSPZUZ\\Shivakumar et al. - 2011 - The debt market relevance of management earnings f.pdf:application/pdf},
}

@misc{araci_finbert_2019,
	title = {{FinBERT}: {Financial} {Sentiment} {Analysis} with {Pre}-trained {Language} {Models}},
	shorttitle = {{FinBERT}},
	url = {http://arxiv.org/abs/1908.10063},
	abstract = {Financial sentiment analysis is a challenging task due to the specialized language and lack of labeled data in that domain. General-purpose models are not effective enough because of the specialized language used in a financial context. We hypothesize that pre-trained language models can help with this problem because they require fewer labeled examples and they can be further trained on domain-specific corpora. We introduce FinBERT, a language model based on BERT, to tackle NLP tasks in the financial domain. Our results show improvement in every measured metric on current state-of-the-art results for two financial sentiment analysis datasets. We find that even with a smaller training set and fine-tuning only a part of the model, FinBERT outperforms state-of-the-art machine learning methods.},
	urldate = {2024-03-26},
	publisher = {arXiv},
	author = {Araci, Dogu},
	month = aug,
	year = {2019},
	note = {arXiv:1908.10063 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: This thesis is submitted in partial fulfillment for the degree of Master of Science in Information Studies: Data Science, University of Amsterdam. June 25, 2019},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\EYSYCXZS\\1908.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\3QS3FWEC\\Araci - 2019 - FinBERT Financial Sentiment Analysis with Pre-tra.pdf:application/pdf},
}

@article{altman_financial_1968,
	title = {Financial {Ratios}, {Discriminant} {Analysis} and the {Prediction} of {Corporate} {Bankruptcy}},
	volume = {23},
	copyright = {© 1968 the American Finance Association},
	issn = {1540-6261},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.1968.tb00843.x},
	doi = {10.1111/j.1540-6261.1968.tb00843.x},
	language = {en},
	number = {4},
	urldate = {2024-03-26},
	journal = {The Journal of Finance},
	author = {Altman, Edward I.},
	year = {1968},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1540-6261.1968.tb00843.x},
	pages = {589--609},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\X3QNYYWY\\j.1540-6261.1968.tb00843.html:text/html},
}

@misc{s_and_p_gics_2024,
	title = {{GICS}® - {Global} {Industry} {Classification} {Standard}},
	url = {https://www.msci.com/our-solutions/indexes/gics},
	abstract = {GICS® is a common global classification standard used by thousands of market participants across all major groups involved in the investment process.},
	language = {en-US},
	urldate = {2024-03-27},
	author = {{S and P} and {MSCI}},
	month = mar,
	year = {2024},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\JW4BLWMD\\gics.html:text/html},
}

@misc{kozlov_us_2022,
	title = {{US} public companies classification},
	url = {https://www.kaggle.com/datasets/aramacus/usa-public-companies},
	abstract = {over 3000 public company descriptions with sector and industry labels},
	language = {en},
	urldate = {2024-03-27},
	author = {Kozlov, Alex},
	month = sep,
	year = {2022},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\4MLFH9IJ\\usa-public-companies.html:text/html},
}

@misc{le_distributed_2014,
	title = {Distributed {Representations} of {Sentences} and {Documents}},
	url = {http://arxiv.org/abs/1405.4053},
	doi = {10.48550/arXiv.1405.4053},
	abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Le, Quoc V. and Mikolov, Tomas},
	month = may,
	year = {2014},
	note = {arXiv:1405.4053 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ijyli\\Zotero\\storage\\UW7SX6KF\\Le and Mikolov - 2014 - Distributed Representations of Sentences and Docum.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\9VEWGM6P\\1405.html:text/html},
}

@misc{beltagy_longformer_2020,
	title = {Longformer: {The} {Long}-{Document} {Transformer}},
	shorttitle = {Longformer},
	url = {http://arxiv.org/abs/2004.05150},
	doi = {10.48550/arXiv.2004.05150},
	abstract = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
	month = dec,
	year = {2020},
	note = {arXiv:2004.05150 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Version 2 introduces the Longformer-Encoder-Decoder (LED) model},
	file = {arXiv Fulltext PDF:C\:\\Users\\ijyli\\Zotero\\storage\\V6M7U8UY\\Beltagy et al. - 2020 - Longformer The Long-Document Transformer.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\36GDEED9\\2004.html:text/html},
}

@book{gunning_technique_1952,
	title = {The {Technique} of {Clear} {Writing}},
	isbn = {978-7-00-001419-0},
	language = {en},
	publisher = {McGraw-Hill},
	author = {Gunning, Robert},
	year = {1952},
	note = {Google-Books-ID: ofI0AAAAMAAJ},
}

@misc{kantos_comparative_2022,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Comparative {Analysis} of {NLP} {Approaches} for {Earnings} {Calls}},
	url = {https://papers.ssrn.com/abstract=4210529},
	doi = {10.2139/ssrn.4210529},
	abstract = {The field of natural language processing (NLP) has evolved significantly in recent years. In this chapter we consider two leading and well-established methodologies, namely, those due to Loughran McDonald, and FinBERT. We then contrast our approach to these two approaches and compare our performance against these methods which are considered to be benchmarks. We use S\&P 500 market data for our investigations and describe the results obtained following our strategies. Our main consideration is the Earnings Calls for the S\&P 500 stocks. We vindicate our findings and present the performance of our trading and fund management strategy which shows better results.},
	language = {en},
	urldate = {2024-04-03},
	author = {Kantos, Christopher and Joldzic, Dan and Mitra, Gautam and Thi Hoang, Kieu},
	month = sep,
	year = {2022},
	keywords = {Earnings Calls, Natural Language Processing, Sentiment Analysis},
	file = {Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\SJNCI2MC\\Kantos et al. - 2022 - Comparative Analysis of NLP Approaches for Earning.pdf:application/pdf},
}

@article{price_earnings_2012,
	title = {Earnings conference calls and stock returns: {The} incremental informativeness of textual tone},
	volume = {36},
	issn = {0378-4266},
	shorttitle = {Earnings conference calls and stock returns},
	url = {https://www.sciencedirect.com/science/article/pii/S0378426611002901},
	doi = {10.1016/j.jbankfin.2011.10.013},
	abstract = {Quarterly earnings conference calls are becoming a more pervasive tool for corporate disclosure. However, the extent to which the market embeds information contained in the tone (i.e. sentiment) of conference call wording is unknown. Using computer aided content analysis, we examine the incremental informativeness of quarterly earnings conference calls and the corresponding market reaction. We find that conference call linguistic tone is a significant predictor of abnormal returns and trading volume. Furthermore, conference call tone dominates earnings surprises over the 60 trading days following the call. The question and answer portion of the call has incremental explanatory power for the post-earnings-announcement drift and this significance is primarily concentrated in firms that do not pay dividends, illustrating differences in investor behavior based on the level of cash flow uncertainty. Additionally, we find that a context specific linguistic dictionary is more powerful than a more widely used general dictionary (Harvard IV-4 Psychosocial).},
	number = {4},
	urldate = {2024-04-03},
	journal = {Journal of Banking \& Finance},
	author = {Price, S. McKay and Doran, James S. and Peterson, David R. and Bliss, Barbara A.},
	month = apr,
	year = {2012},
	keywords = {Disclosure, Textual analysis, Conference calls, Content analysis, Post-earnings-announcement drift, Stock returns},
	pages = {992--1011},
	file = {ScienceDirect Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\CD8GDQQ4\\S0378426611002901.html:text/html},
}

@misc{noauthor_general_nodate,
	title = {General {Inquirer} {Categories}},
	url = {https://inquirer.sites.fas.harvard.edu/homecat.htm},
	urldate = {2024-04-04},
	file = {General Inquirer Categories:C\:\\Users\\ijyli\\Zotero\\storage\\XFVYW2FX\\homecat.html:text/html},
}

@misc{noauthor_inquirer_nodate,
	title = {Inquirer {Home} {Page}},
	url = {https://inquirer.sites.fas.harvard.edu/Home.html},
	urldate = {2024-04-04},
	file = {Inquirer Home Page:C\:\\Users\\ijyli\\Zotero\\storage\\MAJRQIS4\\Home.html:text/html},
}

@misc{spacy_spacy_2024,
	title = {{spaCy} · {Industrial}-strength {Natural} {Language} {Processing} in {Python}},
	url = {https://spacy.io/},
	abstract = {spaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more.},
	language = {en},
	urldate = {2024-05-08},
	author = {{spaCy}},
	year = {2024},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\26BCZ9KM\\spacy.io.html:text/html},
}

@article{chawla_smote_2002,
	title = {{SMOTE}: {Synthetic} {Minority} {Over}-sampling {Technique}},
	volume = {16},
	issn = {1076-9757},
	shorttitle = {{SMOTE}},
	url = {http://arxiv.org/abs/1106.1813},
	doi = {10.1613/jair.953},
	abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
	urldate = {2024-05-08},
	journal = {Journal of Artificial Intelligence Research},
	author = {Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
	month = jun,
	year = {2002},
	note = {arXiv:1106.1813 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	pages = {321--357},
	file = {arXiv Fulltext PDF:C\:\\Users\\ijyli\\Zotero\\storage\\NS4TL7QC\\Chawla et al. - 2002 - SMOTE Synthetic Minority Over-sampling Technique.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\DYQXUCX3\\1106.html:text/html},
}

@misc{kipf_semi-supervised_2017,
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1609.02907},
	doi = {10.48550/arXiv.1609.02907},
	abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
	urldate = {2024-05-08},
	publisher = {arXiv},
	author = {Kipf, Thomas N. and Welling, Max},
	month = feb,
	year = {2017},
	note = {arXiv:1609.02907 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Published as a conference paper at ICLR 2017},
	file = {arXiv Fulltext PDF:C\:\\Users\\ijyli\\Zotero\\storage\\I9TRC3LI\\Kipf and Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\R7DRSVA3\\1609.html:text/html},
}

@misc{hamilton_inductive_2018,
	title = {Inductive {Representation} {Learning} on {Large} {Graphs}},
	url = {http://arxiv.org/abs/1706.02216},
	doi = {10.48550/arXiv.1706.02216},
	abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
	urldate = {2024-05-08},
	publisher = {arXiv},
	author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
	month = sep,
	year = {2018},
	note = {arXiv:1706.02216 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
	annote = {Comment: Published in NIPS 2017; version with full appendix and minor corrections},
	file = {arXiv Fulltext PDF:C\:\\Users\\ijyli\\Zotero\\storage\\NAYEGEKY\\Hamilton et al. - 2018 - Inductive Representation Learning on Large Graphs.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\AJY8CVAU\\1706.html:text/html},
}

@misc{deep_graph_library_deep_2024,
	title = {Deep {Graph} {Library}},
	url = {https://www.dgl.ai/},
	urldate = {2024-05-09},
	author = {{Deep Graph Library}},
	year = {2024},
}
