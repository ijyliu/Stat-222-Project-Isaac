#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
A GraphSage GCN is trained across a number of timesteps (here indexed by
 
\begin_inset Formula $r$
\end_inset

) which progressively mix information from the embeddings of more and more
 distant neighbors.
 Creating these embeddings for a given timestep is a two-step process.
\end_layout

\begin_layout Standard
First, in the AGG step, the embeddings of the node's direct neighbors are
 aggregated.
 Using what is referred to as pool or pooling aggregation, for node 
\begin_inset Formula $h$
\end_inset

, with neighborhood 
\begin_inset Formula $N(i)$
\end_inset

, at timestep 
\begin_inset Formula $r$
\end_inset

,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
h_{N(i)}^{(r)}=max[(\sigma(W_{pool}h_{j}^{(r-1)}+b),\forall j\in N(i)]
\]

\end_inset


\end_layout

\begin_layout Standard
meaning the neighboring embeddings from the previous timestep are fed through
 a fully connected neural network (weights 
\begin_inset Formula $W_{pool}$
\end_inset

, bias 
\begin_inset Formula $b$
\end_inset

, ReLU activation 
\begin_inset Formula $\sigma)$
\end_inset

, and then the elementwise maximum across the new neighbor representations
 is taken as the aggregated vector.
\end_layout

\begin_layout Standard
Next, during the update (or COMBINE) step, we concatenate the representation
 of our node of interest from the previous timestep with this aggregated
 vector, then apply another set of weights and activation to get a fully
 updated node representation.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
h_{i}^{(r)}=\sigma(W^{(r)}*CONCAT[h_{i}^{(r-1)},h_{N(i)}^{(r)}])
\]

\end_inset


\end_layout

\begin_layout Standard
Note that in our implementation the update step has no activation 
\begin_inset Formula $\sigma$
\end_inset

.
\end_layout

\begin_layout Standard
The operations for each timestep ultimately represent a layer of the neural
 network - a SAGEConv layer, of which we have 3.
 For our classification task, the last of these produces output of size
 9 (number of classes) for each node, while we use vectors of size 32 for
 intermediate 
\begin_inset Formula $h$
\end_inset

 values.
 The entire process is trained end-to-end with cross entropy loss.
 We perform weight decay at rate 5e-4 and train for 100 epochs at a learning
 rate of 0.01.
\end_layout

\end_body
\end_document
